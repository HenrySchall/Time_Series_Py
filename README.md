## Time Series

![Gere uma Imagem 02-02-2025 at 21-57-42](https://github.com/user-attachments/assets/bb106d4f-0fa1-4ecc-9ba3-48bca0610d2a)

> A Time Series is a set of observations ordered in time or a particular slice of an unknown stochastic process. Mathematically: Y = Tdt + Szt + et.

* Trend (Tdt): Gradual changes in the long term (population growth).
* Seasonality (Szt): upward and downward oscillations that always occur in a given period (higher electricity bills in winter).
* Residuals (et): shows upward and downward movements in the series after removing the trend or seasonal effect (sequence of random variables).

![plot](https://github.com/user-attachments/assets/6cde76a5-8419-4d3c-b32b-1630c27b36a5)

> Time series studies can be used for future predictions, description of serial behavior, and analysis of periodicity, trends, or even the process that generates the series. They are divided into two types:

- Univariate = only one variable changes over time
- Multivariate = more than one variable changes over time****

### Initial Concepts

> Stochastic Process -> is a collection of random variables defined in the same probability space (process generating a series of variables). The description of a stochastic process is done through a joint probability distribution (which is very complex to do), so we usually describe it through the functions:
- $ùúá(ùë°)=ùê∏{ùëç(ùë°)}$ -> Average
- $ùúé^2(ùë°)=ùëâùëéùëü{ùëç(ùë°)}$ -> Variance 
- $ùõæ(ùë°1,ùë°2)=ùê∂ùëúùë£{ùëç(ùë°1),ùëç(ùë°2)}$ -> Autocovariance

![estocastico](https://github.com/user-attachments/assets/d1a7faa1-0cad-46f2-bf2c-b369e13209c2)

> Stationarity -> is when a time series presents all its statistical characteristics constant over time

- Weak Stationarity = when the statistical properties are constant over time, E(x) = U, Var(x) = ùúé¬≤, COV(X,X-n) = k (covariance between observations at different points in time depends on the specific time at which they occurred). In the literature, stationarity generally means weak stationarity.
- Strong Stationarity = also called strict stationarity, is when the joint probability function is invariant over time, that is, the individual distributions are the same for all "ts". Therefore, the covariance depends only on the distance between the observations and not on the specific time at which they occurred.

> Autocorrelation -> is the correlation of certain previous periods with the current period, that is, the degree of serial dependence. Each period of this type of correlation is called lag and its representation is made by the Autocorrelation Function (ACF) and the Partial Autocorrelation Function (PAF), both of which compare the present value with the past values ‚Äã‚Äãof the series. The difference between them is that the CAF analyzes both direct and indirect correlation, while the PAF only analyzes direct correlation. So we can say that the CAF sees the direct correlation of the month of January in March and also the indirect correlation that the month of January had in February, which also had in March, while the PAF only the correlation of January in March. This analysis is done because it is the essential assumption for creating efficient forecasts of a series.

![FAC](https://github.com/user-attachments/assets/4623a946-6427-4bc2-aadc-d8219df93db9)

![FACP](https://github.com/user-attachments/assets/9d577631-0da2-4101-b695-cfa4f35d2fc5)

> White Noise -> is when the error of a time series follows a normal distribution, that is, a purely random process.
- E(Xt)=0
- Var(Xt)=ùúé2

![Captura de tela 2025-01-30 132421](https://github.com/user-attachments/assets/0fbabff6-f692-48ad-bc84-bea27f7f30ae)

> Random Walk -> is the sum of small stochastic fluctuations (stochastic trend). Mathematically: ùëçùë°=ùëç(ùë°‚àí1)+et

![Captura de tela 2025-01-30 132324](https://github.com/user-attachments/assets/bf7ce3a1-560b-45ad-9d1c-459cea90fe26)

> Transformation and Smoothing -> These are techniques that seek to make the series as close as possible to a normal distribution. Transforming the value of the variables or smoothing the trend and/or seasonality of the series. Among all the existing techniques we can mention:

1) Log Transformation
2) Exponential Transformation
3) Box-Cox Transformation
4) Exponential Moving Average Smoothing (EMA) - Short-term
5) Simple Moving Average Smoothing (SMA) - Long-term

> Differentiation -> Differentiation seeks to transform a non-stationary series into a stationary one, by means of the difference of two consecutive periods

![Sem T√≠tulo-1](https://github.com/user-attachments/assets/390abc00-d4aa-41bf-be96-6ec3eeaf7684)

#### Univariate time series models:
Linear models:
- Autoregressive models (AR)
- Moving average models (MA)
- Autoregressive and moving average models (ARMA)
- Autoregressive integrated and moving average models (ARIMA)
- Long time dependency or long memory models (ARFIMA)
- Autoregressive integrated and moving average models with seasonality (SARIMA)

Nonlinear models:
- Autoregressive with threshold (TAR)
- Autoregressive with smooth transition (STAR)
- Markov regime switching (MSM)
- Autoregressive artificial neural networks (AR-ANN)

Structure:
- Autoregressive (AR): indicates that the variable is regressed on its previous values.
- Integrated (I): indicates that the data values ‚Äã‚Äãwere replaced with the difference between their values ‚Äã‚Äãand the previous values ‚Äã‚Äã(differencing).
- Moving average (MA): Indicates that the regression error is a linear combination of the error terms of the past values.
- 
Coding: (p, d, q) Parameter d can only be an integer, if we were working with an ARFIMA Model, the parameter d can be fractional
- p = order of autoregression.
- d = degree of differentiation.
- q = order of the moving average.

> When we add seasonality, in addition to the Arima coding (p, d, q), we include the coding for Seasonality (P, D, Q). Then a SARIMA model is defined by: (p, d, q)(P, D, Q)
Examples:

- ARFIMA model: (1, 0.25, 1)
- ARIMA model: (2, 1, 1)
- AR model: (1, 0, 0)
- MA model (0, 0, 3)
- I model: (0, 2, 0)
- ARMA model: (4, 0, 1)
- SARIMA model: (1, 1, 2)(2, 0, 1)

Akaike's Information Criterion (AIC) and the Bayesian Information Criterion (BIC)
> In more advanced models, the autocorrelation and partial autocorrelation functions are not informative for defining the order of the models, so an information criterion is used. An information criterion is a way of finding the ideal number of parameters for a model. To understand it, keep in mind that, with each additional regressor, the sum of the residuals will not increase; it will often decrease. The reduction occurs at the cost of more regressors. To balance the reduction in errors and the increase in the number of regressors, the information criterion associates a penalty with this increase. Therefore, its equation has two parts: the first measures the quality of the model's fit to the data, while the second part is called the penalty function since it penalizes models with many parameters. Therefore, given all the combinations of models, we look for the one with the lowest AIC.







#### Testes Estat√≠sticos

##### Teste de Kolmogorov-Smirnov
> Qualifica a m√°xima diferen√ßa absoluta entre a fun√ß√£o de distribui√ß√£o da amostra e a fun√ß√£o de distribui√ß√£o acumulada da distribui√ß√£o de refer√™ncia (geramente distribui√ß√£o normal), ou seja, ele qualifica dist√¢ncia entre duas amostras (compara√ß√£o entre elas).

- *H0: A amostra segue a distribui√ß√£o de refer√™ncia*
- *H1: A amostra n√£o segue a distribui√ß√£o de refer√™ncia*

##### Teste de Anderson-Darling 
> Testa se uma fun√ß√£o de distribui√ß√£o acumulada f(x), pode ser candidata a ser um fun√ß√£o de distribui√ß√£o acumulada de uma amostra aleat√≥ria;

- *H0: A amostra tem distribui√ß√£o de f(x)*
- *H1: A amostra n√£o tem distribui√ß√£o f(x)*

##### Teste de Shapiro Wilk 
> O teste Shapiro Wilk segue a seguinte equa√ß√£o descrita abaixo. Sendo que xi s√£o os valores da amostra ordenados, no qual valores menores que W s√£o evid√™ncias de que os dados s√£o normais.

![Captura de tela 2024-07-04 191812](https://github.com/HenrySchall/Time-Series/assets/96027335/c9789639-2602-44bb-a9f3-491b92b65310)

> J√° o termo b √© determinado pela seguinte equa√ß√£o:

![Captura de tela 2024-07-04 192115](https://github.com/HenrySchall/Time-Series/assets/96027335/c2594f21-082f-4f6d-9293-66b45b0125fb)

> onde ai s√£o constantes geradas pelas m√©dias, vari√¢ncias e covari√¢ncias das estat√≠sticas de ordem de uma amostra de tamanho n de uma distribui√ß√£o normal (tabela da normal).

Estat√≠stica de teste:
- *H0: A amostra segue uma distribui√ß√£o normal (W-obtido < W-cr√≠tico)*
- *H1: A amostra n√£o segue uma distribui√ß√£o normal (W-obtido > W-cr√≠tico)*

![img46](https://github.com/HenrySchall/Time-Series/assets/96027335/64ca5aa1-d601-44b1-8d21-16ec79400211)

##### Teste de Jarque-Bera
> Verifica se os erros s√£o um Ru√≠do Branco, ou seja, seguem uma distribui√ß√£o normal. O teste se baseia nos res√≠duos do m√©todo dos m√≠nimos quadrados. Para sua realiza√ß√£o o teste necessita dos c√°lculos da assimetria (skewness) e da curtose (kurtosis) da amostra, dado pela seguinte f√≥rmula:
 
![Captura de tela 2024-07-04 193133](https://github.com/HenrySchall/Time-Series/assets/96027335/fe76cc80-fa40-46c8-8357-7e19e49339a5)

> onde n e o n√∫mero de observa√ß√µes (ou graus de liberdade geral); S √© aassimetria da amostra; e K √© a curtose da amostra

![Captura de tela 2024-07-04 193243](https://github.com/HenrySchall/Time-Series/assets/96027335/b24d6ca3-6e20-44ed-a3d6-5004c3646bd6)

$\widehat{u3}$ e $\widehat{u4}$ s√£o as estimativas do terceiro e quarto momentos, respectivamente; $\bar{x}$ a m√©dia da amostra, e $ùúé^2$ √© a estimativa do segundo momento, a vari√¢ncia.

- *H0: res√≠duos s√£o normalmente distribu√≠dos*
- *H1: res√≠duos n√£o s√£o normalmente distribu√≠dos*

#### Resumo:

|Teste|Quando usar|Pr√≥s|Contras|Cen√°rios n√£o indicados|
|---|---|---|---|---|
|Shapiro-Wilk|Pequenas amostras (sens√≠vel a pequenas desvios da normalidade)|Sens√≠vel a pequenas desvios da normalidade (adequado para amostras pequenas|Pode ser menos potente em amostras maiores|Dados com distribui√ß√£o fortemente bimodal ou multimodal|
|Kolmogorov-Smirov|Amostras grandes (teste n√£o param√©trico)|N√£o requer suposi√ß√µes sobre os par√¢metros da distribui√ß√£o (adequado para amostras grandes)|Menos sens√≠vel a pequenos desvios (menos potente em amostras pequenas)|Sens√≠vel a desvios nas caudas da distribui√ß√£o|
|Anderson Darling|Verifica√ß√£o geral de normalidade|Sensibilidade a desvios em caudas e simetria (fornece estat√≠stica de teste e valores cr√≠ticos)|Menos sens√≠vel a desvios pequenos|N√£o √© recomendado para amostras muito pequenas|
|Jaque-Bera|Verifica√ß√£o geral de normalidade em amostras grandes|Combina informa√ß√µes sobre simetria e curtose (adequado para amostras grandes)|Menos sens√≠vel a desvios pequenos|Sens√≠vel a desvios nas caudas da distribui√ß√£o| 
  
##### Teste de Ader√™ncia
> Este teste √© utilizado quando deseja-se validar a hip√≥tese que um conjunto de dados √© gerado por uma determinada distribui√ß√£o de probabilidade.

- *H0: segue o modelo proposto*
- *H1: n√£o segue o modelo proposto*
  
##### Teste de Indeped√™ncia
> Este teste √© utilizado quando deseja-se validar a hip√≥tese de independ√™ncia entre duas vari√°veis aleat√≥rias. Se por exemplo, existe a funl√ßao de probabilidade conjunta das duas vari√°veis aleat√≥rias, pode-se verificar se para todos os poss√≠veis valores das vari√°vies, o produto das probabilidades margianis √© igual √† probabilidade conjunto.

- *H0: as vari√°veis aleat√≥rias s√£o independentes*
- *H1: as vari√°veis aleat√≥rias n√£o s√£o independentes*

##### Teste de Homogeneidade
> Esse teste √© utilizado quando deseja-se validar a hip√≥tese de que uma vari√°vel aleat√≥ria apresenta comportamento similar, ou homog√™neo, em rela√ß√£o √†s suas v√°rias subpopula√ß√µes. Este teste apresenta a mesma mec√¢nica do Teste de Independ√™ncia, mas uma distin√ß√£o importante se refere √† forma como as amostras s√£o coletadas. No Teste de homogeneidade fixa-se o tamanho da amostra em cada uma das subpopula√ß√µes e, ent√£o, seleciona-se uma amostra de cada uma delas.

- *H0: As subpopula√ß√µes das vari√°veis aleat√≥rias s√£o homog√™neas*
- *H1: As subpopula√ß√µes das vari√°veis aleat√≥rias n√£o s√£o homog√™neas*

#### Coeficientes de Correla√ß√£o
> Os coeficientes de correla√ß√£o verificam a exist√™ncia e o grau de associa√ß√£o entre dois conjuntos de dados.

##### Coeficiente Pearson 
> Estabelecer o n√≠vel de rela√ß√£o linear entre duas vari√°veis. Em outras palavras, mede em grau e o sentido (crescente/decrescente) da associa√ß√£o linear entre duas vari√°veis. Ele sempre estar√° entre ‚àí1,00 e +1,00, tendo o sinal a fun√ß√£o de indicar a dire√ß√£o do movimento, ou seja, positivo (rela√ß√£o direta) e negativa (rela√ß√£o inversa) e o valor do coeficiente, a fun√ß√£o de indicar a for√ßa da correla√ß√£o, onde nos intervalos:
> - (+0,90; +1,00) ou (‚àí1,00; ‚àí0,90) = correla√ß√£o muito forte
> - (+0,60; +0,90) ou (‚àí0,90; ‚àí0,60) = correla√ß√£o forte
> - (+0,30; +0,60) ou (‚àí0,60; ‚àí0,30) = correla√ß√£o moderada
> - (0,00; +0,30) ou (‚àí0,30; 0,00) = correla√ß√£o fraca
>
> Graficamente:

![3](https://github.com/HenrySchall/Time-Series/assets/96027335/5391579e-90f0-4ed2-92a0-b95c6068591f)

> Sua equa√ß√£o √© definida pela seguinte f√≥rmula:

![1](https://github.com/HenrySchall/Time-Series/assets/96027335/8f4dd7f6-e82b-4bf0-a06d-58400ede1060)

> Lembrando que o coeficiente de correla√ß√£o populacional √© dado por:

![2](https://github.com/HenrySchall/Time-Series/assets/96027335/6e39a2b7-4bfd-4d30-987e-bca3e8c5c8d8)

> Exemplo: A tabela abaixo apresenta 15 observa√ß√µes, com o tempo de entrega (em minutos) e a dist√¢ncia de entrega de um TelePizza.

|Tempo|Dist√¢ncia|
|---|---|
|40|688|
|21|215|
|14|255|
|20|462|
|24|448|
|29|776|
|15|200|
|19|132|
|10|36|
|35|770|
|18|140|
|52|810|
|19|450|
|20|635|
|11|150|

> Calculando os valores obtemos o seguinte resultado:

![4](https://github.com/HenrySchall/Time-Series/assets/96027335/a224016f-5d0a-4b84-ac79-6b8f5dae6ae1)

> Conclui-se que existe uma rela√ß√£o linear forte e positiva entre as vari√°veis. Todavia o coeficiente de correla√ß√£o de Pearson √© apenas uma estimativa do coeficiente de correla√ß√£o populacional, pois √© calculado com base em uma amostra aleat√≥ria de ùëõ pares de dados. Sendo assim a amostra observada pode apresentar correla√ß√£o, mas a popula√ß√£o n√£o, neste caso, tem-se um problema de infer√™ncia, pois o fato de r‚â†0 n√£o √© garantia de ùúå‚â†0. Para resolver esse problema, utiliza-se da estat√≠stica de teste T-student, definido pela equa√ß√£o abaixo, para verificar se realmente existe correla√ß√£o linear entre as vari√°veis:

![5](https://github.com/HenrySchall/Time-Series/assets/96027335/84310f44-b3d8-477d-9e71-1ec81dcbb21a)

> Onde ùë° segue uma distribui√ß√£o ùë°‚àíùëÜùë°ùë¢ùëëùëíùëõùë° com ùëõ‚àí2 graus de liberdade e regido pelas seguintes hip√≥teses:

- *H0: A correla√ß√£o entre as vari√°veis √© zero (ùúå = 0)*
- *H1: A correla√ß√£o entre as vari√°veis n√£o √© zero (ùúå ‚â† 0)*

![6](https://github.com/HenrySchall/Time-Series/assets/96027335/ccc5a428-cea3-4a8f-a773-c6b454b87f28)

> A partir da estat√≠stica ùë° com 13 graus de liberdade, os pontos cr√≠ticos s√£o ¬±2,1604. Portanto, rejeita-se ùêªùëú ao n√≠vel de signific√¢ncia de 5%. Sendo assim a correla√ß√£o entre o tempo de entrega e a dist√¢ncia percorrida √© diferente de zero, ent√£o, existe uma rela√ß√£o linear e positiva entre as vari√°veis da ordem de ùëü = 0,8216.

##### Coeficiente Spearman
> O coeficiente de correla√ß√£o de Spearman, ou rho de Spearman, √© uma medida n√£o param√©trica da correla√ß√£o (associa√ß√£o) entre duas vari√°veis ordinais. Ao contr√°rio do coeficiente de correla√ß√£o de Pearson, que mede a for√ßa e a dire√ß√£o da rela√ß√£o linear entre duas vari√°veis, o coeficiente de Spearman avalia a intensidade (o qu√£o bem) √© a rela√ß√£o entre duas vari√°veis. O coeficiente de correla√ß√£o de Spearman (ùúå) √© calculado utilizando a seguinte f√≥rmula:

![20](https://github.com/HenrySchall/Time-Series/assets/96027335/65f56f8e-31b3-4d4f-a4d0-b7e692b2fd44)

#### Interpreta√ß√£o:
- œÅ=1 indica uma perfeita correla√ß√£o positiva.
- œÅ=‚àí1 indica uma perfeita correla√ß√£o negativa.
- œÅ=0 indica aus√™ncia de correla√ß√£o.

> Exemplo: Dados os valores da tabela abaixo:

![9](https://github.com/HenrySchall/Time-Series/assets/96027335/68db17b8-2b31-41c9-b1d1-cf241b30ee55)

> Calculando os valores obtemos o seguinte resultado:

![12](https://github.com/HenrySchall/Time-Series/assets/96027335/1c332788-5570-48e1-8d17-2b3b36c61aff)
 
> Utilizando-se da mesma equa√ß√£o estat√≠stica do teste T-student. Teremos as seguintes hip√≥teses:

- *H0: A correla√ß√£o entre as vari√°veis √© zero*
- *H1: A correla√ß√£o entre as vari√°veis n√£o √© zero*

> A partir da estat√≠stica ùë°‚àíùëÜùë°ùë¢ùëëùëíùëõùë° com 11 graus de liberdade, os pontos cr√≠ticos s√£o ¬±2,2010. Portanto, rejeita-se ùêªùëú ao n√≠vel de signific√¢ncia de 5%. Sendo assim a correla√ß√£o entre as vari√°veis ùëã e ùëå √© diferente 
de zero, ent√£o, existe uma rela√ß√£o n√£o-linear e negativa de ordem ùëü= ‚àí0,9698. 

##### Coeficiente Kendall 
> O coeficiente de correla√ß√£o de Kendall √© uma medida estat√≠stica utilizada para avaliar a associa√ß√£o entre duas vari√°veis ordinais, exatamente igual ao coeficiente de correla√ß√£o de Spearman, a difenre√ßa √© que ele mede a correla√ß√£o de concord√¢ncia, enquanto Spearman, mede a correla√ß√£o de postos. Sendo particularmente √∫til quando as vari√°veis em quest√£o n√£o assumem necessariamente distribui√ß√µes normais. O coeficiente de correla√ß√£o de Kendall (œÑ) √© definido pela seguinte f√≥rmula:

![124](https://github.com/HenrySchall/Time-Series/assets/96027335/ac68ba9a-1c0d-4cab-a892-7d9ab87cc400)

> No qual, ùëõ √© o n√∫mero de elementos aos quais atribui-se postos, ùëÜ √© a soma da vari√°vel ùëå √† direita que s√£o superiores menos o n√∫mero de postos √† direita que s√£o inferiores.

#### Interpreta√ß√£o:
- œÑ=1 indica uma perfeita concord√¢ncia.
- œÑ=‚àí1 indica uma perfeita discord√¢ncia.
- œÑ=0 indica aus√™ncia de associa√ß√£o entre as vari√°veis.

> Para o c√°lculo do coeficiente de correla√ß√£o por postos de Kendall ordena-se inicialmente uma das vari√°veis em ordem crescente de postos e o S correspondente a cada elemento ser√° obtido fazendo o n√∫mero de elementos 
cujo posto √© superior ao que se est√° calculando menos o n√∫mero de elementos cujo posto √© inferior ao mesmo. Para verificar a signific√¢ncia do valor observado do coeficiente ùúè de Kendall, para ùëõ‚â§10 deve-se consultar a tabela abaixo.

![125](https://github.com/HenrySchall/Time-Series/assets/96027335/9a2f16b5-e667-4d3c-a8dd-01d3ed678409)

> Para ùëõ>10, pode utilizar a estat√≠stica de teste:

![128](https://github.com/HenrySchall/Time-Series/assets/96027335/275d3eda-ef7e-453a-86d1-a4918ef935ff)

> Exemplo: Dados os valores da tabela abaixo:



> Calculando os valores obtemos o seguinte resultado:



> Tendo as seguintes hip√≥teses:

- *H0: A correla√ß√£o entre as vari√°veis √© zero (ùúè=0)*
- *H1: A correla√ß√£o entre as vari√°veis n√£o √© zero (ùúè‚â†0)*

>  A partir da estat√≠stica ùëÅùëúùëüùëöùëéùëô ùëùùëéùëëùëü√£ùëú, os pontos cr√≠ticos s√£o ¬±1,96. Portanto, rejeita-se ùêªùëú ao n√≠vel de signific√¢ncia de 5%. Sendo assim a correla√ß√£o entre as vari√°veis ùëã e ùëå √© diferente de zero, ent√£o, existe uma 
rela√ß√£o n√£o-linear e negativa de ordem ùúè=‚àí0,7692.

*Observa√ß√£o: Pode-se fazer uma compara√ß√£o entre coeficiente de correla√ß√£o de Spearman e o coeficiente de correla√ß√£o por postos de Kendall. Os valores num√©ricos n√£o s√£o iguais, quando calculados para os mesmos pares de postos, e n√£o s√£o compar√°veis numericamente. Contudo, pelo fato de utilizarem a mesma quantidade de informa√ß√£o contida nos dados, ambos t√™m o mesmo poder de detectar a exist√™ncia de associa√ß√£o na popula√ß√£o, e rejeitar√£o a hip√≥tese nula para um mesmo n√≠vel de signific√¢ncia.*

### Bibliographical References:
- Introduction to Time Series, by Gustavo Rocha Silva
- Practical Time Series Analysis: Prediction with Statistics and Machine Learning, by Aileen Nielsen
- Elementary Statistics: Picturing the World Hardcover, by Ron Larson (Auteur), Betsy Farber
- Time Series Econometrics, by Rodrigo de Losso da Silveira Bueno




