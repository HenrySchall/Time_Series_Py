## Time Series
### Introduction
> A Time Series is a set of observations ordered in time or a particular slice of an unknown stochastic process. Mathematically: Y = Tdt + Szt + et.

* Trend (Tdt): Gradual changes in the long term (population growth).
* Seasonality (Szt): upward and downward oscillations that always occur in a given period (higher electricity bills in winter).
* Residuals (et): shows upward and downward movements in the series after removing the trend or seasonal effect (sequence of random variables).

![plot](https://github.com/user-attachments/assets/6cde76a5-8419-4d3c-b32b-1630c27b36a5)

> Time series studies can be used for future predictions, description of serial behavior, and analysis of periodicity, trends, or even the process that generates the series. They are divided into two types:

- Univariate = only one variable changes over time
- Multivariate = more than one variable changes over time****

### Initial Concepts

> Stochastic Process -> is a collection of random variables defined in the same probability space (process generating a series of variables). The description of a stochastic process is done through a joint probability distribution (which is very complex to do), so we usually describe it through the functions:
- $ğœ‡(ğ‘¡)=ğ¸{ğ‘(ğ‘¡)}$ -> Average
- $ğœ^2(ğ‘¡)=ğ‘‰ğ‘ğ‘Ÿ{ğ‘(ğ‘¡)}$ -> Variance 
- $ğ›¾(ğ‘¡1,ğ‘¡2)=ğ¶ğ‘œğ‘£{ğ‘(ğ‘¡1),ğ‘(ğ‘¡2)}$ -> Autocovariance

![estocastico](https://github.com/user-attachments/assets/d1a7faa1-0cad-46f2-bf2c-b369e13209c2)

> Stationarity -> is when a time series presents all its statistical characteristics constant over time

- Weak Stationarity = when the statistical properties are constant over time, E(x) = U, Var(x) = ğœÂ², COV(X,X-n) = k (covariance between observations at different points in time depends on the specific time at which they occurred). In the literature, stationarity generally means weak stationarity.
- Strong Stationarity = also called strict stationarity, is when the joint probability function is invariant over time, that is, the individual distributions are the same for all "ts". Therefore, the covariance depends only on the distance between the observations and not on the specific time at which they occurred.

> Autocorrelation -> is the correlation of certain previous periods with the current period, that is, the degree of serial dependence. Each period of this type of correlation is called lag and its representation is made by the Autocorrelation Function (ACF) and the Partial Autocorrelation Function (PAF), both of which compare the present value with the past values â€‹â€‹of the series. The difference between them is that the CAF analyzes both direct and indirect correlation, while the PAF only analyzes direct correlation. So we can say that the CAF sees the direct correlation of the month of January in March and also the indirect correlation that the month of January had in February, which also had in March, while the PAF only the correlation of January in March. This analysis is done because it is the essential assumption for creating efficient forecasts of a series.

![FAC](https://github.com/user-attachments/assets/4623a946-6427-4bc2-aadc-d8219df93db9)

![FACP](https://github.com/user-attachments/assets/9d577631-0da2-4101-b695-cfa4f35d2fc5)

> White Noise -> is when the error of a time series follows a normal distribution, that is, a purely random process.
- E(Xt)=0
- Var(Xt)=ğœ2

![Captura de tela 2025-01-30 132421](https://github.com/user-attachments/assets/0fbabff6-f692-48ad-bc84-bea27f7f30ae)

> Random Walk -> is the sum of small stochastic fluctuations (stochastic trend). Mathematically: ğ‘ğ‘¡=ğ‘(ğ‘¡âˆ’1)+et

![Captura de tela 2025-01-30 132324](https://github.com/user-attachments/assets/bf7ce3a1-560b-45ad-9d1c-459cea90fe26)

> Transformation and Smoothing -> These are techniques that seek to make the series as close as possible to a normal distribution. Transforming the value of the variables or smoothing the trend and/or seasonality of the series. Among all the existing techniques we can mention:

1) Log Transformation
2) Exponential Transformation
3) Box-Cox Transformation
4) Exponential Moving Average Smoothing (EMA) - Short-term
5) Simple Moving Average Smoothing (SMA) - Long-term

> Differentiation -> Differentiation seeks to transform a non-stationary series into a stationary one, by means of the difference of two consecutive periods

![Sem TÃ­tulo-1](https://github.com/user-attachments/assets/390abc00-d4aa-41bf-be96-6ec3eeaf7684)

#### Univariate time series models:
Linear models:
- Autoregressive models (AR)
- Moving average models (MA)
- Autoregressive and moving average models (ARMA)
- Autoregressive integrated and moving average models (ARIMA)
- Long time dependency or long memory models (ARFIMA)
- Autoregressive integrated and moving average models with seasonality (SARIMA)

Nonlinear models:
- Autoregressive with threshold (TAR)
- Autoregressive with smooth transition (STAR)
- Markov regime switching (MSM)
- Autoregressive artificial neural networks (AR-ANN)

Structure:
- Autoregressive (AR): indicates that the variable is regressed on its previous values.
- Integrated (I): indicates that the data values â€‹â€‹were replaced with the difference between their values â€‹â€‹and the previous values â€‹â€‹(differencing).
- Moving average (MA): Indicates that the regression error is a linear combination of the error terms of the past values.
- 
Coding: (p, d, q) Parameter d can only be an integer, if we were working with an ARFIMA Model, the parameter d can be fractional
- p = order of autoregression.
- d = degree of differentiation.
- q = order of the moving average.

> When we add seasonality, in addition to the Arima coding (p, d, q), we include the coding for Seasonality (P, D, Q). Then a SARIMA model is defined by: (p, d, q)(P, D, Q)
Examples:

- ARFIMA model: (1, 0.25, 1)
- ARIMA model: (2, 1, 1)
- AR model: (1, 0, 0)
- MA model (0, 0, 3)
- I model: (0, 2, 0)
- ARMA model: (4, 0, 1)
- SARIMA model: (1, 1, 2)(2, 0, 1)

Akaike's Information Criterion (AIC) and the Bayesian Information Criterion (BIC)
> In more advanced models, the autocorrelation and partial autocorrelation functions are not informative for defining the order of the models, so an information criterion is used. An information criterion is a way of finding the ideal number of parameters for a model. To understand it, keep in mind that, with each additional regressor, the sum of the residuals will not increase; it will often decrease. The reduction occurs at the cost of more regressors. To balance the reduction in errors and the increase in the number of regressors, the information criterion associates a penalty with this increase. Therefore, its equation has two parts: the first measures the quality of the model's fit to the data, while the second part is called the penalty function since it penalizes models with many parameters. Therefore, given all the combinations of models, we look for the one with the lowest AIC.







#### Testes EstatÃ­sticos

##### Teste de Kolmogorov-Smirnov
> Qualifica a mÃ¡xima diferenÃ§a absoluta entre a funÃ§Ã£o de distribuiÃ§Ã£o da amostra e a funÃ§Ã£o de distribuiÃ§Ã£o acumulada da distribuiÃ§Ã£o de referÃªncia (geramente distribuiÃ§Ã£o normal), ou seja, ele qualifica distÃ¢ncia entre duas amostras (comparaÃ§Ã£o entre elas).

- *H0: A amostra segue a distribuiÃ§Ã£o de referÃªncia*
- *H1: A amostra nÃ£o segue a distribuiÃ§Ã£o de referÃªncia*

##### Teste de Anderson-Darling 
> Testa se uma funÃ§Ã£o de distribuiÃ§Ã£o acumulada f(x), pode ser candidata a ser um funÃ§Ã£o de distribuiÃ§Ã£o acumulada de uma amostra aleatÃ³ria;

- *H0: A amostra tem distribuiÃ§Ã£o de f(x)*
- *H1: A amostra nÃ£o tem distribuiÃ§Ã£o f(x)*

##### Teste de Shapiro Wilk 
> O teste Shapiro Wilk segue a seguinte equaÃ§Ã£o descrita abaixo. Sendo que xi sÃ£o os valores da amostra ordenados, no qual valores menores que W sÃ£o evidÃªncias de que os dados sÃ£o normais.

![Captura de tela 2024-07-04 191812](https://github.com/HenrySchall/Time-Series/assets/96027335/c9789639-2602-44bb-a9f3-491b92b65310)

> JÃ¡ o termo b Ã© determinado pela seguinte equaÃ§Ã£o:

![Captura de tela 2024-07-04 192115](https://github.com/HenrySchall/Time-Series/assets/96027335/c2594f21-082f-4f6d-9293-66b45b0125fb)

> onde ai sÃ£o constantes geradas pelas mÃ©dias, variÃ¢ncias e covariÃ¢ncias das estatÃ­sticas de ordem de uma amostra de tamanho n de uma distribuiÃ§Ã£o normal (tabela da normal).

EstatÃ­stica de teste:
- *H0: A amostra segue uma distribuiÃ§Ã£o normal (W-obtido < W-crÃ­tico)*
- *H1: A amostra nÃ£o segue uma distribuiÃ§Ã£o normal (W-obtido > W-crÃ­tico)*

![img46](https://github.com/HenrySchall/Time-Series/assets/96027335/64ca5aa1-d601-44b1-8d21-16ec79400211)

##### Teste de Jarque-Bera
> Verifica se os erros sÃ£o um RuÃ­do Branco, ou seja, seguem uma distribuiÃ§Ã£o normal. O teste se baseia nos resÃ­duos do mÃ©todo dos mÃ­nimos quadrados. Para sua realizaÃ§Ã£o o teste necessita dos cÃ¡lculos da assimetria (skewness) e da curtose (kurtosis) da amostra, dado pela seguinte fÃ³rmula:
 
![Captura de tela 2024-07-04 193133](https://github.com/HenrySchall/Time-Series/assets/96027335/fe76cc80-fa40-46c8-8357-7e19e49339a5)

> onde n e o nÃºmero de observaÃ§Ãµes (ou graus de liberdade geral); S Ã© aassimetria da amostra; e K Ã© a curtose da amostra

![Captura de tela 2024-07-04 193243](https://github.com/HenrySchall/Time-Series/assets/96027335/b24d6ca3-6e20-44ed-a3d6-5004c3646bd6)

$\widehat{u3}$ e $\widehat{u4}$ sÃ£o as estimativas do terceiro e quarto momentos, respectivamente; $\bar{x}$ a mÃ©dia da amostra, e $ğœ^2$ Ã© a estimativa do segundo momento, a variÃ¢ncia.

- *H0: resÃ­duos sÃ£o normalmente distribuÃ­dos*
- *H1: resÃ­duos nÃ£o sÃ£o normalmente distribuÃ­dos*

#### Resumo:

|Teste|Quando usar|PrÃ³s|Contras|CenÃ¡rios nÃ£o indicados|
|---|---|---|---|---|
|Shapiro-Wilk|Pequenas amostras (sensÃ­vel a pequenas desvios da normalidade)|SensÃ­vel a pequenas desvios da normalidade (adequado para amostras pequenas|Pode ser menos potente em amostras maiores|Dados com distribuiÃ§Ã£o fortemente bimodal ou multimodal|
|Kolmogorov-Smirov|Amostras grandes (teste nÃ£o paramÃ©trico)|NÃ£o requer suposiÃ§Ãµes sobre os parÃ¢metros da distribuiÃ§Ã£o (adequado para amostras grandes)|Menos sensÃ­vel a pequenos desvios (menos potente em amostras pequenas)|SensÃ­vel a desvios nas caudas da distribuiÃ§Ã£o|
|Anderson Darling|VerificaÃ§Ã£o geral de normalidade|Sensibilidade a desvios em caudas e simetria (fornece estatÃ­stica de teste e valores crÃ­ticos)|Menos sensÃ­vel a desvios pequenos|NÃ£o Ã© recomendado para amostras muito pequenas|
|Jaque-Bera|VerificaÃ§Ã£o geral de normalidade em amostras grandes|Combina informaÃ§Ãµes sobre simetria e curtose (adequado para amostras grandes)|Menos sensÃ­vel a desvios pequenos|SensÃ­vel a desvios nas caudas da distribuiÃ§Ã£o| 
  
##### Teste de AderÃªncia
> Este teste Ã© utilizado quando deseja-se validar a hipÃ³tese que um conjunto de dados Ã© gerado por uma determinada distribuiÃ§Ã£o de probabilidade.

- *H0: segue o modelo proposto*
- *H1: nÃ£o segue o modelo proposto*
  
##### Teste de IndepedÃªncia
> Este teste Ã© utilizado quando deseja-se validar a hipÃ³tese de independÃªncia entre duas variÃ¡veis aleatÃ³rias. Se por exemplo, existe a funlÃ§ao de probabilidade conjunta das duas variÃ¡veis aleatÃ³rias, pode-se verificar se para todos os possÃ­veis valores das variÃ¡vies, o produto das probabilidades margianis Ã© igual Ã  probabilidade conjunto.

- *H0: as variÃ¡veis aleatÃ³rias sÃ£o independentes*
- *H1: as variÃ¡veis aleatÃ³rias nÃ£o sÃ£o independentes*

##### Teste de Homogeneidade
> Esse teste Ã© utilizado quando deseja-se validar a hipÃ³tese de que uma variÃ¡vel aleatÃ³ria apresenta comportamento similar, ou homogÃªneo, em relaÃ§Ã£o Ã s suas vÃ¡rias subpopulaÃ§Ãµes. Este teste apresenta a mesma mecÃ¢nica do Teste de IndependÃªncia, mas uma distinÃ§Ã£o importante se refere Ã  forma como as amostras sÃ£o coletadas. No Teste de homogeneidade fixa-se o tamanho da amostra em cada uma das subpopulaÃ§Ãµes e, entÃ£o, seleciona-se uma amostra de cada uma delas.

- *H0: As subpopulaÃ§Ãµes das variÃ¡veis aleatÃ³rias sÃ£o homogÃªneas*
- *H1: As subpopulaÃ§Ãµes das variÃ¡veis aleatÃ³rias nÃ£o sÃ£o homogÃªneas*

#### Coeficientes de CorrelaÃ§Ã£o
> Os coeficientes de correlaÃ§Ã£o verificam a existÃªncia e o grau de associaÃ§Ã£o entre dois conjuntos de dados.

##### Coeficiente Pearson 
> Estabelecer o nÃ­vel de relaÃ§Ã£o linear entre duas variÃ¡veis. Em outras palavras, mede em grau e o sentido (crescente/decrescente) da associaÃ§Ã£o linear entre duas variÃ¡veis. Ele sempre estarÃ¡ entre âˆ’1,00 e +1,00, tendo o sinal a funÃ§Ã£o de indicar a direÃ§Ã£o do movimento, ou seja, positivo (relaÃ§Ã£o direta) e negativa (relaÃ§Ã£o inversa) e o valor do coeficiente, a funÃ§Ã£o de indicar a forÃ§a da correlaÃ§Ã£o, onde nos intervalos:
> - (+0,90; +1,00) ou (âˆ’1,00; âˆ’0,90) = correlaÃ§Ã£o muito forte
> - (+0,60; +0,90) ou (âˆ’0,90; âˆ’0,60) = correlaÃ§Ã£o forte
> - (+0,30; +0,60) ou (âˆ’0,60; âˆ’0,30) = correlaÃ§Ã£o moderada
> - (0,00; +0,30) ou (âˆ’0,30; 0,00) = correlaÃ§Ã£o fraca
>
> Graficamente:

![3](https://github.com/HenrySchall/Time-Series/assets/96027335/5391579e-90f0-4ed2-92a0-b95c6068591f)

> Sua equaÃ§Ã£o Ã© definida pela seguinte fÃ³rmula:

![1](https://github.com/HenrySchall/Time-Series/assets/96027335/8f4dd7f6-e82b-4bf0-a06d-58400ede1060)

> Lembrando que o coeficiente de correlaÃ§Ã£o populacional Ã© dado por:

![2](https://github.com/HenrySchall/Time-Series/assets/96027335/6e39a2b7-4bfd-4d30-987e-bca3e8c5c8d8)

> Exemplo: A tabela abaixo apresenta 15 observaÃ§Ãµes, com o tempo de entrega (em minutos) e a distÃ¢ncia de entrega de um TelePizza.

|Tempo|DistÃ¢ncia|
|---|---|
|40|688|
|21|215|
|14|255|
|20|462|
|24|448|
|29|776|
|15|200|
|19|132|
|10|36|
|35|770|
|18|140|
|52|810|
|19|450|
|20|635|
|11|150|

> Calculando os valores obtemos o seguinte resultado:

![4](https://github.com/HenrySchall/Time-Series/assets/96027335/a224016f-5d0a-4b84-ac79-6b8f5dae6ae1)

> Conclui-se que existe uma relaÃ§Ã£o linear forte e positiva entre as variÃ¡veis. Todavia o coeficiente de correlaÃ§Ã£o de Pearson Ã© apenas uma estimativa do coeficiente de correlaÃ§Ã£o populacional, pois Ã© calculado com base em uma amostra aleatÃ³ria de ğ‘› pares de dados. Sendo assim a amostra observada pode apresentar correlaÃ§Ã£o, mas a populaÃ§Ã£o nÃ£o, neste caso, tem-se um problema de inferÃªncia, pois o fato de râ‰ 0 nÃ£o Ã© garantia de ğœŒâ‰ 0. Para resolver esse problema, utiliza-se da estatÃ­stica de teste T-student, definido pela equaÃ§Ã£o abaixo, para verificar se realmente existe correlaÃ§Ã£o linear entre as variÃ¡veis:

![5](https://github.com/HenrySchall/Time-Series/assets/96027335/84310f44-b3d8-477d-9e71-1ec81dcbb21a)

> Onde ğ‘¡ segue uma distribuiÃ§Ã£o ğ‘¡âˆ’ğ‘†ğ‘¡ğ‘¢ğ‘‘ğ‘’ğ‘›ğ‘¡ com ğ‘›âˆ’2 graus de liberdade e regido pelas seguintes hipÃ³teses:

- *H0: A correlaÃ§Ã£o entre as variÃ¡veis Ã© zero (ğœŒ = 0)*
- *H1: A correlaÃ§Ã£o entre as variÃ¡veis nÃ£o Ã© zero (ğœŒ â‰  0)*

![6](https://github.com/HenrySchall/Time-Series/assets/96027335/ccc5a428-cea3-4a8f-a773-c6b454b87f28)

> A partir da estatÃ­stica ğ‘¡ com 13 graus de liberdade, os pontos crÃ­ticos sÃ£o Â±2,1604. Portanto, rejeita-se ğ»ğ‘œ ao nÃ­vel de significÃ¢ncia de 5%. Sendo assim a correlaÃ§Ã£o entre o tempo de entrega e a distÃ¢ncia percorrida Ã© diferente de zero, entÃ£o, existe uma relaÃ§Ã£o linear e positiva entre as variÃ¡veis da ordem de ğ‘Ÿ = 0,8216.

##### Coeficiente Spearman
> O coeficiente de correlaÃ§Ã£o de Spearman, ou rho de Spearman, Ã© uma medida nÃ£o paramÃ©trica da correlaÃ§Ã£o (associaÃ§Ã£o) entre duas variÃ¡veis ordinais. Ao contrÃ¡rio do coeficiente de correlaÃ§Ã£o de Pearson, que mede a forÃ§a e a direÃ§Ã£o da relaÃ§Ã£o linear entre duas variÃ¡veis, o coeficiente de Spearman avalia a intensidade (o quÃ£o bem) Ã© a relaÃ§Ã£o entre duas variÃ¡veis. O coeficiente de correlaÃ§Ã£o de Spearman (ğœŒ) Ã© calculado utilizando a seguinte fÃ³rmula:

![20](https://github.com/HenrySchall/Time-Series/assets/96027335/65f56f8e-31b3-4d4f-a4d0-b7e692b2fd44)

#### InterpretaÃ§Ã£o:
- Ï=1 indica uma perfeita correlaÃ§Ã£o positiva.
- Ï=âˆ’1 indica uma perfeita correlaÃ§Ã£o negativa.
- Ï=0 indica ausÃªncia de correlaÃ§Ã£o.

> Exemplo: Dados os valores da tabela abaixo:

![9](https://github.com/HenrySchall/Time-Series/assets/96027335/68db17b8-2b31-41c9-b1d1-cf241b30ee55)

> Calculando os valores obtemos o seguinte resultado:

![12](https://github.com/HenrySchall/Time-Series/assets/96027335/1c332788-5570-48e1-8d17-2b3b36c61aff)
 
> Utilizando-se da mesma equaÃ§Ã£o estatÃ­stica do teste T-student. Teremos as seguintes hipÃ³teses:

- *H0: A correlaÃ§Ã£o entre as variÃ¡veis Ã© zero*
- *H1: A correlaÃ§Ã£o entre as variÃ¡veis nÃ£o Ã© zero*

> A partir da estatÃ­stica ğ‘¡âˆ’ğ‘†ğ‘¡ğ‘¢ğ‘‘ğ‘’ğ‘›ğ‘¡ com 11 graus de liberdade, os pontos crÃ­ticos sÃ£o Â±2,2010. Portanto, rejeita-se ğ»ğ‘œ ao nÃ­vel de significÃ¢ncia de 5%. Sendo assim a correlaÃ§Ã£o entre as variÃ¡veis ğ‘‹ e ğ‘Œ Ã© diferente 
de zero, entÃ£o, existe uma relaÃ§Ã£o nÃ£o-linear e negativa de ordem ğ‘Ÿ= âˆ’0,9698. 

##### Coeficiente Kendall 
> O coeficiente de correlaÃ§Ã£o de Kendall Ã© uma medida estatÃ­stica utilizada para avaliar a associaÃ§Ã£o entre duas variÃ¡veis ordinais, exatamente igual ao coeficiente de correlaÃ§Ã£o de Spearman, a difenreÃ§a Ã© que ele mede a correlaÃ§Ã£o de concordÃ¢ncia, enquanto Spearman, mede a correlaÃ§Ã£o de postos. Sendo particularmente Ãºtil quando as variÃ¡veis em questÃ£o nÃ£o assumem necessariamente distribuiÃ§Ãµes normais. O coeficiente de correlaÃ§Ã£o de Kendall (Ï„) Ã© definido pela seguinte fÃ³rmula:

![124](https://github.com/HenrySchall/Time-Series/assets/96027335/ac68ba9a-1c0d-4cab-a892-7d9ab87cc400)

> No qual, ğ‘› Ã© o nÃºmero de elementos aos quais atribui-se postos, ğ‘† Ã© a soma da variÃ¡vel ğ‘Œ Ã  direita que sÃ£o superiores menos o nÃºmero de postos Ã  direita que sÃ£o inferiores.

#### InterpretaÃ§Ã£o:
- Ï„=1 indica uma perfeita concordÃ¢ncia.
- Ï„=âˆ’1 indica uma perfeita discordÃ¢ncia.
- Ï„=0 indica ausÃªncia de associaÃ§Ã£o entre as variÃ¡veis.

> Para o cÃ¡lculo do coeficiente de correlaÃ§Ã£o por postos de Kendall ordena-se inicialmente uma das variÃ¡veis em ordem crescente de postos e o S correspondente a cada elemento serÃ¡ obtido fazendo o nÃºmero de elementos 
cujo posto Ã© superior ao que se estÃ¡ calculando menos o nÃºmero de elementos cujo posto Ã© inferior ao mesmo. Para verificar a significÃ¢ncia do valor observado do coeficiente ğœ de Kendall, para ğ‘›â‰¤10 deve-se consultar a tabela abaixo.

![125](https://github.com/HenrySchall/Time-Series/assets/96027335/9a2f16b5-e667-4d3c-a8dd-01d3ed678409)

> Para ğ‘›>10, pode utilizar a estatÃ­stica de teste:

![128](https://github.com/HenrySchall/Time-Series/assets/96027335/275d3eda-ef7e-453a-86d1-a4918ef935ff)

> Exemplo: Dados os valores da tabela abaixo:



> Calculando os valores obtemos o seguinte resultado:



> Tendo as seguintes hipÃ³teses:

- *H0: A correlaÃ§Ã£o entre as variÃ¡veis Ã© zero (ğœ=0)*
- *H1: A correlaÃ§Ã£o entre as variÃ¡veis nÃ£o Ã© zero (ğœâ‰ 0)*

>  A partir da estatÃ­stica ğ‘ğ‘œğ‘Ÿğ‘šğ‘ğ‘™ ğ‘ğ‘ğ‘‘ğ‘ŸÃ£ğ‘œ, os pontos crÃ­ticos sÃ£o Â±1,96. Portanto, rejeita-se ğ»ğ‘œ ao nÃ­vel de significÃ¢ncia de 5%. Sendo assim a correlaÃ§Ã£o entre as variÃ¡veis ğ‘‹ e ğ‘Œ Ã© diferente de zero, entÃ£o, existe uma 
relaÃ§Ã£o nÃ£o-linear e negativa de ordem ğœ=âˆ’0,7692.

*ObservaÃ§Ã£o: Pode-se fazer uma comparaÃ§Ã£o entre coeficiente de correlaÃ§Ã£o de Spearman e o coeficiente de correlaÃ§Ã£o por postos de Kendall. Os valores numÃ©ricos nÃ£o sÃ£o iguais, quando calculados para os mesmos pares de postos, e nÃ£o sÃ£o comparÃ¡veis numericamente. Contudo, pelo fato de utilizarem a mesma quantidade de informaÃ§Ã£o contida nos dados, ambos tÃªm o mesmo poder de detectar a existÃªncia de associaÃ§Ã£o na populaÃ§Ã£o, e rejeitarÃ£o a hipÃ³tese nula para um mesmo nÃ­vel de significÃ¢ncia.*








